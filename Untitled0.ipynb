{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS(Hadoop Components)"
      ],
      "metadata": {
        "id": "QdsLDHPaVzB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "INkHWF5ZMPTg",
        "outputId": "cc19bfc5-d736-44ca-cf24-3b4020aa80bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 1: hviud\n",
            "Node 2: vhdsv\n",
            "Node 3: iudsg\n",
            "Node 4: vdgsv\n",
            "Node 5: iudsg\n",
            "Node 6: vdvbv\n",
            "Node 7: ifgfd\n",
            "Node 8: isuvb\n",
            "Node 9: dsvkj\n",
            "Node 10: sbdiy\n",
            "Node 11: uvsvk\n",
            "Node 12: jkcbv\n",
            "Node 13: ciuvg\n",
            "Node 14: jsvsi\n",
            "Node 15: vbcvk\n",
            "Node 16: mcvip\n",
            "Node 17: uvhjh\n",
            "Node 18: fduyf\n",
            "Node 19: gduhd\n",
            "Node 20: hvfdj\n",
            "Node 21: hchvd\n",
            "Node 22: cfdfd\n",
            "Node 23: jbcfb\n",
            "Node 24: dcjgd\n",
            "Node 25: fcduh\n",
            "Node 26: dmnxz\n",
            "Node 27: cliyg\n",
            "Node 28: ifuhf\n",
            "Node 29: mvvlg\n",
            "Node 30: viuff\n",
            "Node 31: jkbfi\n",
            "Node 32: ufefk\n",
            "Node 33: jdsbv\n",
            "Node 34: divud\n",
            "Node 35: gvkjv\n",
            "Node 36: b\n"
          ]
        }
      ],
      "source": [
        "#Develop\n",
        "def split_file(file_content,chunk_size):\n",
        "    chunks=[file_content[i:i+chunk_size]\n",
        "             for i in range(0,len(file_content),chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Usage\n",
        "file_content=\"hviudvhdsviudsgvdgsviudsgvdvbvifgfdisuvbdsvkjsbdiyuvsvkjkcbvciuvgjsvsivbcvkmcvipuvhjhfduyfgduhdhvfdjhchvdcfdfdjbcfbdcjgdfcduhdmnxzcliygifuhfmvvlgviuffjkbfiufefkjdsbvdivudgvkjvb\"\n",
        "chunk_size=5\n",
        "chunks=split_file(file_content,chunk_size)\n",
        "\n",
        "for i,chunk in enumerate(chunks):\n",
        "    print(f\"Node {i+1}: {chunk}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Engine"
      ],
      "metadata": {
        "id": "sdtZtstuWQmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Big Data Tools Overview\").getOrCreate()\n",
        "data=[(\"Alice\",29),(\"Bob\",34),(\"Catherine\",27)]\n",
        "columns=[\"Name\",\"Age\"]\n",
        "df=spark.createDataFrame(data,schema=columns)\n",
        "print(\"Sample DataFrame:\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZgvkrnMWPFt",
        "outputId": "8b075a9d-fc4c-4ecc-bac2-021eb6bbc51f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Sample DataFrame:\n",
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 29|\n",
            "|      Bob| 34|\n",
            "|Catherine| 27|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map Reduce\n"
      ],
      "metadata": {
        "id": "kpV17iZFZk4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simulate mapreduce for word count\n",
        "from collections import Counter\n",
        "\n",
        "#sample dataset\n",
        "data=[\"Big Data ecosystem\",\"Data architecture\",\"Big Data LPU\"]\n",
        "\n",
        "#map reduce\n",
        "mapped=[]\n",
        "for line in data:\n",
        "    words=line.split()\n",
        "    mapped.extend([(word,1) for word in words])\n",
        "\n",
        "\n",
        "reduced=Counter()\n",
        "for key,value in mapped:\n",
        "    reduced[key]+=value\n",
        "print(\"Word Count Using MapReduce Simulation:\")\n",
        "for word,count in reduced.items():\n",
        "    print(f\"{word}:{count}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6GTi1IpZkSw",
        "outputId": "e53f455e-591b-4fa5-fc02-dbf648d236f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count Using MapReduce Simulation:\n",
            "Big:2\n",
            "Data:3\n",
            "ecosystem:1\n",
            "architecture:1\n",
            "LPU:1\n"
          ]
        }
      ]
    }
  ]
}